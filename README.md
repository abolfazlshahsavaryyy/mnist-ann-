# MNIST — Neural Network from Scratch

### A simple artificial neural network for MNIST implemented from scratch with gradient descent and mathematics

This project is an Artificial Neural Network (ANN) built from scratch to classify the MNIST handwritten digit dataset.
The focus is on understanding the mathematics behind neural networks — forward pass, loss functions, backpropagation, and weight updates — all implemented manually in Python.

## Key Features
  
    Implemented completely from scratch (no TensorFlow or PyTorch)
    Gradient Descent optimization
    Includes mathematical formulas for loss and gradient updates
    Training with batch and stochastic gradient descent
    Visualizations of training loss and sample predictions
    ui for predict the input number of user input

## Implementation Notes


    architecture: 784 → 128 → 64 → 10 with ReLU activations and softmax output
    Weight initialization with small random values
    Gradient descent with optional momentum
    98% accuracy in text data and 99% on train
    

    عه  
